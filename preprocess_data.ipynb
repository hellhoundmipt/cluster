{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import jsonlines\n",
    "import itertools\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 as pm\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "#from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from random import shuffle\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from mediawiki import MediaWiki\n",
    "from mediawiki import PageError\n",
    "from mediawiki import MediaWikiCategoryTreeError\n",
    "\n",
    "stopwords_ru = set(stopwords.words('russian'))\n",
    "tknzr = TweetTokenizer()\n",
    "morph = pm.MorphAnalyzer()\n",
    "wiki = MediaWiki(url='http://ru.wikipedia.org/w/api.php')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AD', 'AE', 'AF', 'AI', 'AJ', 'AK', 'AQ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:08<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "path = listdir('c')\n",
    "path = sorted(path)[:9]\n",
    "#path = ['AA', 'AD', 'AE']\n",
    "print(path)\n",
    "\n",
    "data = []\n",
    "for p in tqdm(path):\n",
    "    with jsonlines.open('c/' + p, 'r') as f:\n",
    "        for entry in f:\n",
    "            data.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбираемся с категориями. Добавялем категории, который есть в ok, но которых нет в дереве/dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_graph = nx.read_gpickle('sources/graph_gpickle.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 719813/719813 [00:06<00:00, 103830.36it/s]\n"
     ]
    }
   ],
   "source": [
    "links = [link for link in jsonlines.open('links/link_directed.txt', 'r')]\n",
    "categories_id_dict = {}\n",
    "id_categories_dict = {}\n",
    "for link in tqdm(links):\n",
    "    id_categories_dict[link['node_1_id']] = re.sub(r'^Категория:', '', link['node_1_title'])\n",
    "    id_categories_dict[link['node_2_id']] = re.sub(r'^Категория:', '', link['node_2_title'])\n",
    "    \n",
    "    categories_id_dict[re.sub(r'^Категория:', '', link['node_1_title'])] = link['node_1_id']\n",
    "    categories_id_dict[re.sub(r'^Категория:', '', link['node_2_title'])] = link['node_2_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 116584/116584 [00:01<00:00, 73219.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in trange(len(data)):\n",
    "    categories = data[i]['categories']\n",
    "    categories_nu = set()\n",
    "    for c in categories:\n",
    "        c = re.sub(r'\\u200e', '', c)\n",
    "        c = re.sub(r'\\xa0', ' ', c)\n",
    "        categories_nu.add(re.sub(r'\\|.*', '', c))\n",
    "    data[i]['categories'] = categories_nu\n",
    "    \n",
    "categories = [d['categories'] for d in data]    \n",
    "categories = set(list(itertools.chain(*categories)))\n",
    "\n",
    "categories_dict = {c: 0 for c in categories}\n",
    "for d in data:\n",
    "    for c in d['categories']:\n",
    "        if categories_dict.get(c) is not None:\n",
    "            categories_dict[c] += 1\n",
    "            \n",
    "ok_set = {c for c in categories if categories_dict[c] > 5 and categories_dict[c] < 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 14857/14857 [00:00<00:00, 1058227.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed = []\n",
    "cat_graph_nodes = {id_categories_dict[cat] for cat in cat_graph.nodes()}\n",
    "for c in tqdm(ok_set):\n",
    "    if c not in cat_graph_nodes:\n",
    "        missed.append(c)\n",
    "\n",
    "len(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 557/557 [15:44<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doomed = set()\n",
    "restored = set()\n",
    "\n",
    "for miss in tqdm(missed):\n",
    "    flag = True\n",
    "    while flag:\n",
    "        try:\n",
    "            res = wiki.categorytree(category=miss, depth=1)\n",
    "            flag = False\n",
    "        except PageError:\n",
    "            try:\n",
    "                res = wiki.categorytree(category='Категория:'+miss, depth=1)\n",
    "                flag = False\n",
    "            except PageError:\n",
    "                doomed.add(miss)\n",
    "                flag = False\n",
    "        except MediaWikiCategoryTreeError:\n",
    "            sleep(3)\n",
    "            \n",
    "    parents = res[list(res.keys())[0]]['parent-categories']\n",
    "    parents = [re.sub(r'^Категория:', '', p) for p in parents]\n",
    "    for parent in parents:\n",
    "        if parent in cat_graph_nodes:\n",
    "            flag = True\n",
    "            new_node_id = max(cat_graph.nodes()) + 1\n",
    "            cat_graph.add_edge(categories_id_dict[parent], new_node_id)\n",
    "            id_categories_dict[new_node_id] = miss\n",
    "            categories_id_dict[miss] = new_node_id\n",
    "            restored.add((parent, miss))\n",
    "            \n",
    "    if not flag:\n",
    "        doomed.add(miss)\n",
    "\n",
    "\n",
    "len(doomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sources/id_cat.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(id_categories_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sources/cat_id.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(categories_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 116584/116584 [00:01<00:00, 87796.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 14697/14697 [03:24<00:00, 72.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ok_set = {c for c in ok_set if c not in doomed}\n",
    "ok = sorted(ok_set)\n",
    "with open(\"sources/accepted_categories.txt\", mode=\"w\", encoding=\"utf-8\") as output:\n",
    "    for c in ok:\n",
    "        output.write(c + '\\n')\n",
    "\n",
    "article_category_dict = {}\n",
    "for d in tqdm(data):\n",
    "    cats = [c for c in d['categories'] if c in ok_set]\n",
    "    if len(cats) > 0:\n",
    "        article_category_dict[d['id']] = cats\n",
    "with open(\"sources/article_cat.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(article_category_dict))\n",
    "\n",
    "category_article_dict = {c: [d['id'] for d in data if c in d['categories']] for c in tqdm(ok_set)}\n",
    "with open(\"sources/cat_article.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(category_article_dict))\n",
    "    \n",
    "nx.write_gpickle(cat_graph, 'sources/graph_gpickle.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем тексты (лемматизированные и просто \"очищенные\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 116584/116584 [05:07<00:00, 378.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "ids = article_category_dict.keys()\n",
    "\n",
    "with jsonlines.open('sources/texts.jl', 'w') as output:\n",
    "    for d in tqdm(data):\n",
    "        if d['id'] in ids:\n",
    "            text = re.sub(r'\\n+', '. ', d['text'])\n",
    "            text = re.sub(r'\\xa0', ' ', text)\n",
    "            tokens = tknzr.tokenize(text=text)\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stopwords_ru]     #  if t.isalnum()\n",
    "            text = ' '.join(tokens)\n",
    "            output.write({'text': text, 'id': d['id']})\n",
    "            words.extend(sorted(set([t for t in tokens if t.isalpha()])))\n",
    "\n",
    "print(\"Done\")\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1231449/1231449 [06:24<00:00, 3200.89it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_forms = {}\n",
    "for w in tqdm(words):\n",
    "    nf = morph.parse(w)[0].normal_form\n",
    "    normal_forms[w] = nf\n",
    "    \n",
    "with open(\"sources/normal_forms.json\", mode=\"w\") as output:\n",
    "    output.write(json.dumps(normal_forms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96794it [01:02, 1545.66it/s]\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open('sources/texts.jl', 'r') as file:\n",
    "    with jsonlines.open('sources/normalized_texts.jl', 'w') as output:\n",
    "        for f in tqdm(file):\n",
    "            id = f['id']\n",
    "            text = f['text']\n",
    "            tokens = re.split(r' ', text)\n",
    "            tokens = [normal_forms.get(t) for t in tokens if normal_forms.get(t) is not None]\n",
    "            text = ' '.join(tokens)\n",
    "            output.write({'text': text, 'id': id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем tf-idf и svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {item['id']: item['text'] for item in jsonlines.open('sources/normalized_texts.jl', 'r')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.98, )\n",
    "X_tfidf = vectorizer.fit_transform([text for (id, text) in sorted(texts.items())])\n",
    "np.save(\"sources/tf_idf\", X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=500, random_state=27)\n",
    "X_svd = svd.fit_transform(text_tfidf)\n",
    "np.save(\"sources/svd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
