{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import pymorphy2 as pm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подгатавливаем тексты и теги\n",
    "Читаем из файла данные, избавляемся от ненужных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Легпром', 4), ('События', 2234), ('Мир', 1274), ('Стиль', 1096), ('Ресурсы', 690), ('Россия', 376), ('Оружие', 4120), ('ТВ и радио', 1951), ('Пресса', 1424), ('Книги', 939), ('Первая мировая', 65), ('Внешний вид', 211), ('Хоккей', 381), ('Мемы', 336), ('Все', 450709), ('Следствие и суд', 4165), ('Театр', 882), ('Криминал', 1497), ('Архитектура', 232), ('Движение', 423), ('Молдавия', 333), ('Теннис', 930), ('Явления', 1345), ('Средняя Азия', 936), ('Производители', 36), ('Гаджеты', 1816), ('Инновации', 1), ('Наследие', 14), ('Искусство', 1495), ('Еда', 332), ('Мнения', 359), ('Часы', 355), ('Москва', 1175), ('Аналитика рынка', 28), ('Вещи', 305), ('Туризм', 30), ('Банки', 1876), ('Бокс и ММА', 1379), ('Деньги', 987), ('Кино', 6592), ('История', 203), ('Происшествия', 10858), ('Госрегулирование', 41), ('Coцсети', 2039), ('Культпросвет', 1), ('Госэкономика', 9190), ('Техника', 936), ('Игры', 1954), ('Полиция и спецслужбы', 905), ('Выборы', 4), ('Технологии', 200), ('Вооруженные силы', 2973), ('Футбол', 8678), ('Казахстан', 606), ('Кавказ', 1134), ('Финансы компаний', 142), ('Преступность', 3811), ('Катастрофы', 368), ('Конфликты', 2061), ('Прибалтика', 1043), ('Рынки', 1405), ('Вооружение', 2), ('Люди', 3696), ('Общество', 19437), ('Летние виды', 2392), ('Автобизнес', 242), ('Космос', 2576), ('Страноведение', 10), ('Инструменты', 397), ('Украина', 13376), ('Вкусы', 11), ('Реклама', 232), ('Интернет', 5067), ('Деловой климат', 1598), ('Наука', 5516), ('Политика', 22849), ('Зимние виды', 755), ('Авто', 24), ('Достижения', 368), ('Вирусные ролики', 196), ('Музыка', 3809), ('Регионы', 1304), ('Звери', 1225), ('Крым', 3), ('Компании', 6626), ('Мировой опыт', 6), ('Экология', 32), ('Мировой бизнес', 680), ('Безопасность ', 41), ('Баскетбол', 573), ('Белоруссия', 689)]\n",
      "['События', 'Мир', 'Стиль', 'Ресурсы', 'Россия', 'Оружие', 'ТВ и радио', 'Пресса', 'Книги', 'Внешний вид', 'Хоккей', 'Мемы', 'Следствие и суд', 'Театр', 'Криминал', 'Архитектура', 'Движение', 'Молдавия', 'Теннис', 'Явления', 'Средняя Азия', 'Гаджеты', 'Искусство', 'Еда', 'Мнения', 'Часы', 'Москва', 'Вещи', 'Банки', 'Бокс и ММА', 'Деньги', 'Кино', 'История', 'Происшествия', 'Coцсети', 'Госэкономика', 'Техника', 'Игры', 'Полиция и спецслужбы', 'Технологии', 'Вооруженные силы', 'Футбол', 'Казахстан', 'Кавказ', 'Финансы компаний', 'Преступность', 'Катастрофы', 'Конфликты', 'Прибалтика', 'Рынки', 'Люди', 'Общество', 'Летние виды', 'Автобизнес', 'Космос', 'Инструменты', 'Украина', 'Реклама', 'Интернет', 'Деловой климат', 'Наука', 'Политика', 'Зимние виды', 'Достижения', 'Вирусные ролики', 'Музыка', 'Регионы', 'Звери', 'Компании', 'Мировой бизнес', 'Баскетбол', 'Белоруссия']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:\\Max\\docs_cluster\\lenta\\lenta_data.csv')\n",
    "data = list(zip(df['text'].tolist(), df['tags'].tolist()))\n",
    "tags = sorted(set(df['tags'].tolist()))\n",
    "\n",
    "tags_cnt = list(Counter([d[1] for d in data]).items())\n",
    "tags_accepted = [tag[0] for tag in tags_cnt if tag[0] != 'Все' and tag[1] > 100]\n",
    "print(tags_cnt)\n",
    "print(tags_accepted)\n",
    "\n",
    "texts = [d[0] for d in data if d[1] in tags_accepted]\n",
    "text_tags = [d[1] for d in data if d[1] in tags_accepted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс TextClassifier\n",
    "Сделал, чтобы можно было один раз загнать корпус и после этого играться с параметрами у классифкаторов. Пока получилось не очень, если честно..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, texts, text_tags, selections_num):\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        self.texts = texts\n",
    "        le.fit(list(set(text_tags)))\n",
    "        self.text_tags = le.transform(text_tags)\n",
    "        \n",
    "        self.make_selections(selections_num)\n",
    "        self.__lemmatize()\n",
    "        \n",
    "        \n",
    "    def __lemmatize(self):\n",
    "        print(\"Making vocab\")\n",
    "        morph = pm.MorphAnalyzer()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = []\n",
    "        n = len(self.texts)\n",
    "        for i, text in enumerate(self.texts):\n",
    "            self.__check_percentage(i, n)\n",
    "                \n",
    "            tokens.append([t.lower() for t in tokenizer.tokenize(text) if t.isalnum()])\n",
    "        print('...')\n",
    "\n",
    "        words_set = set().union(*tokens)\n",
    "        print('Vocab length = ' + str(len(words_set)))\n",
    "        vocab = {t: morph.parse(t)[0].normal_form for t in words_set}\n",
    "        print('Finished vocab, lemmatizing')\n",
    "        lemmatized_texts = []\n",
    "        for toks in tokens:\n",
    "            lemmatized_texts.append(' '.join([vocab.get(t) for t in toks if vocab.get(t) is not None]))\n",
    "            \n",
    "        self.texts = lemmatized_texts\n",
    "        print(\"Done\")\n",
    "            \n",
    "        \n",
    "    def make_selections(self, selections_num):\n",
    "        self.selections_num = selections_num\n",
    "        self.corpus_size = len(self.texts)\n",
    "        self.selection_size = self.corpus_size // self.selections_num\n",
    "        \n",
    "    \n",
    "    def preset(self, norm='l2', use_idf=True, max_df=100000, min_df=10, stop_words=True):\n",
    "        self.tfidfTransformer = TfidfTransformer(use_idf=use_idf)\n",
    "        if stop_words:\n",
    "            stopwords_ru = [w for w in stopwords.words(\"russian\")]\n",
    "        else:\n",
    "            stopwords_ru = None\n",
    "            \n",
    "        self.vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, stop_words=stopwords_ru)\n",
    "        \n",
    "        \n",
    "    def __train_n_test(self, clf, clf_name):\n",
    "        print(\"Training \" + clf_name)\n",
    "        models = []\n",
    "        \n",
    "        #Training\n",
    "        for i in range(self.selections_num - 1):\n",
    "            print(\"Selection #\" + str(i))\n",
    "            start = i * self.selection_size\n",
    "            end = start + self.selection_size\n",
    "            \n",
    "            models.append(clf.fit(self.texts[start:end], self.text_tags[start:end]))\n",
    "        \n",
    "        #Testing\n",
    "        start = (self.selections_num - 1) * self.selection_size\n",
    "        test_selection_cnt = self.corpus_size - (self.selections_num - 1) * self.selection_size\n",
    "        test_tags = self.text_tags[start:]\n",
    "\n",
    "        model_predictions = [model.predict(texts[start:]) for model in models]\n",
    "        prediction = []\n",
    "        for i in range(len(model_predictions[0])):\n",
    "            predictions = [p[i] for p in model_predictions]\n",
    "            prediction.append(max(set(predictions), key=predictions.count))\n",
    "\n",
    "\n",
    "        prediction = np.array(prediction)\n",
    "        log_reg_accuracy = accuracy_score(test_tags, prediction)\n",
    "        print(\"Method accuracy = \" + str(log_reg_accuracy))\n",
    "        return models\n",
    "       \n",
    "    \n",
    "    \n",
    "    #Classifiers\n",
    "    def log_reg(self, C=None):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', LogisticRegression(C=C))\n",
    "               ])\n",
    "        \n",
    "        return self.__train_n_test(clf, \"logistic regression\")\n",
    "    \n",
    "    \n",
    "    def random_forest(self, max_depth=None, random_state=None):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', RandomForestClassifier(max_depth=max_depth, random_state=random_state))\n",
    "               ])\n",
    "        \n",
    "        return self.__train_n_test(clf, \"random forest\")\n",
    "    \n",
    "    \n",
    "    def decision_tree(self, max_depth=None, random_state=None):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', DecisionTreeClassifier(max_depth=max_depth, random_state=random_state))\n",
    "               ])\n",
    "        \n",
    "        return self.__train_n_test(clf, \"decision tree\")\n",
    "    \n",
    "    \n",
    "    def radom_gradient(self):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', SGDClassifier())\n",
    "               ])\n",
    "        \n",
    "        return self.__train_n_test(clf, \"random gradient\")\n",
    "    \n",
    "    \n",
    "    def adaboost(self, random_state=None):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', AdaBoostClassifier(random_state=random_state))\n",
    "               ])\n",
    "        \n",
    "        return self.__train_n_test(clf, \"AdaBoostClassifier\")\n",
    "    \n",
    "    \n",
    "    def gaussian(self):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', GaussianNB())\n",
    "               ])\n",
    "\n",
    "        return self.__train_n_test(clf, \"gauss\")\n",
    "    \n",
    "    \n",
    "    def linearSVC(self, C=None):\n",
    "        clf = Pipeline([('vect', self.vectorizer), \n",
    "             ('tfidf', self.tfidfTransformer),\n",
    "             ('clf', LinearSVC(C=C))\n",
    "               ])\n",
    "\n",
    "        return self.__train_n_test(clf, \"linear SVC\")\n",
    "    \n",
    "      \n",
    "        \n",
    "    def __check_percentage(self, i, n):\n",
    "        r = i / n * 100\n",
    "        if r > 10 and  r < 10.0005:\n",
    "            print(str(r) + '%')\n",
    "        elif r > 30 and  r < 30.0005:\n",
    "            print(str(r) + '%')\n",
    "        elif r > 50 and  r < 50.0005:\n",
    "            print(str(r) + '%')\n",
    "        elif r > 70 and  r < 70.0005:\n",
    "            print(str(r) + '%')\n",
    "        elif r > 90 and  r < 90.0005:\n",
    "            print(str(r) + '%')\n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "Вместо того, чтобы писать метод для каждого классификатора, лучше сделать как-то так (позже переделаю)\n",
    "classifiers = [\n",
    "    (\"SGD\", SGDClassifier()),\n",
    "    (\"ASGD\", SGDClassifier(average=True)),\n",
    "    (\"Perceptron\", Perceptron()),\n",
    "    (\"Passive-Aggressive I\", PassiveAggressiveClassifier(loss='hinge',\n",
    "                                                         C=1.0)),\n",
    "    (\"Passive-Aggressive II\", PassiveAggressiveClassifier(loss='squared_hinge',\n",
    "                                                          C=1.0)),\n",
    "    (\"SAG\", LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / X.shape[0]))\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection #0\n",
      "Training random forest\n",
      "Selection #1\n",
      "Training random forest\n",
      "Selection #2\n",
      "Training random forest\n",
      "Selection #3\n",
      "Training random forest\n",
      "Selection #4\n",
      "Training random forest\n",
      "Method accuracy = 0.363979193758\n",
      "_\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier(texts, text_tags, 6)\n",
    "text_classifier.preset()\n",
    "\n",
    "text_classifier.log_reg(C=10)    #лучший результат при C = 10 (~54%)\n",
    "text_classifier.random_forest()   #37% без парамеров\n",
    "text_classifier.radom_gradient()  #быстрее, чем Logistic regression, точность 58%\n",
    "text_classifier.decision_tree() \n",
    "text_classifier.adaboost() \n",
    "text_classifier.gaussian()       #пока не смотрел\n",
    "text_classifier.linearSVC(C=10)  #пока не смотрел\n",
    "\n",
    "print('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Логистическая регрессия__ лучше всего при c = 10, точность *54%*\n",
    "\n",
    "__Random forest__ без параметров *37%*\n",
    "\n",
    "__SGDC__ быстрый, точность *58%*\n",
    "\n",
    "__Решающее дерево__ без параметров *24%*\n",
    "\n",
    "__Adaboost__ *14%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ниже черновик\n",
    "# ы\n",
    "# ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2 as pm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "morph = pm.MorphAnalyzer()\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = []\n",
    "vocab = []\n",
    "for text in texts:\n",
    "    tokens.extend([t.lower() for t in tokenizer.tokenize(text) if t.isalnum()])\n",
    "\n",
    "tokens = set(tokens)\n",
    "vocab = {t: morph.parse(t)[0].normal_form for t in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30759\n",
      "30760\n",
      "0 - 30759\n",
      "30759 - 61518\n",
      "61518 - 92277\n",
      "92277 - 123036\n",
      "123036 - 153795\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(texts)\n",
    "selections_num = 6      # На сколько частей разбиваем датасет\n",
    "selection_size = corpus_size // selections_num\n",
    "print(selection_size)\n",
    "print(corpus_size - (selections_num - 1) * selection_size)\n",
    "\n",
    "\n",
    "for i in range(selections_num - 1):\n",
    "    start = i * selection_size\n",
    "    end = start + selection_size\n",
    "    print(str(start) + ' - ' + str(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируемся на первых пяти датасетах без лемматизации, проверяемся на шестом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection #0\n",
      "Training logistic regression\n",
      "Yay 0\n",
      "Yay 1000\n",
      "Yay 2000\n",
      "Yay 3000\n",
      "10.000325108098442\n",
      "10.003576189082871\n",
      "10.006827270067298\n",
      "Yay 4000\n",
      "Yay 5000\n",
      "Yay 6000\n",
      "Yay 7000\n",
      "Yay 8000\n",
      "Yay 9000\n",
      "30.000975324295325\n",
      "30.004226405279756\n",
      "30.007477486264182\n",
      "Yay 10000\n",
      "Yay 11000\n",
      "Yay 12000\n",
      "Yay 13000\n",
      "Yay 14000\n",
      "Yay 15000\n",
      "50.00162554049221\n",
      "50.00487662147665\n",
      "50.00812770246107\n",
      "Yay 16000\n",
      "Yay 17000\n",
      "Yay 18000\n",
      "Yay 19000\n",
      "Yay 20000\n",
      "Yay 21000\n",
      "70.0022757566891\n",
      "70.00552683767353\n",
      "70.00877791865796\n",
      "Yay 22000\n",
      "Yay 23000\n",
      "Yay 24000\n",
      "Yay 25000\n",
      "Yay 26000\n",
      "Yay 27000\n",
      "90.00292597288599\n",
      "90.0061770538704\n",
      "90.00942813485484\n",
      "Yay 28000\n",
      "Yay 29000\n",
      "Yay 30000\n",
      "Done lemmatization\n",
      "Selection #1\n",
      "Training logistic regression\n",
      "Yay 0\n",
      "Yay 1000\n",
      "Yay 2000\n",
      "Yay 3000\n",
      "10.000325108098442\n",
      "10.003576189082871\n",
      "10.006827270067298\n",
      "Yay 4000\n",
      "Yay 5000\n",
      "Yay 6000\n",
      "Yay 7000\n",
      "Yay 8000\n",
      "Yay 9000\n",
      "30.000975324295325\n",
      "30.004226405279756\n",
      "30.007477486264182\n",
      "Yay 10000\n",
      "Yay 11000\n",
      "Yay 12000\n",
      "Yay 13000\n",
      "Yay 14000\n",
      "Yay 15000\n",
      "50.00162554049221\n",
      "50.00487662147665\n",
      "50.00812770246107\n",
      "Yay 16000\n",
      "Yay 17000\n",
      "Yay 18000\n",
      "Yay 19000\n",
      "Yay 20000\n",
      "Yay 21000\n",
      "70.0022757566891\n",
      "70.00552683767353\n",
      "70.00877791865796\n",
      "Yay 22000\n",
      "Yay 23000\n",
      "Yay 24000\n",
      "Yay 25000\n",
      "Yay 26000\n",
      "Yay 27000\n",
      "90.00292597288599\n",
      "90.0061770538704\n",
      "90.00942813485484\n",
      "Yay 28000\n",
      "Yay 29000\n",
      "Yay 30000\n",
      "Done lemmatization\n",
      "Selection #2\n",
      "Training logistic regression\n",
      "Yay 0\n",
      "Yay 1000\n",
      "Yay 2000\n",
      "Yay 3000\n",
      "10.000325108098442\n",
      "10.003576189082871\n",
      "10.006827270067298\n",
      "Yay 4000\n",
      "Yay 5000\n",
      "Yay 6000\n",
      "Yay 7000\n",
      "Yay 8000\n",
      "Yay 9000\n",
      "30.000975324295325\n",
      "30.004226405279756\n",
      "30.007477486264182\n",
      "Yay 10000\n",
      "Yay 11000\n",
      "Yay 12000\n",
      "Yay 13000\n",
      "Yay 14000\n",
      "Yay 15000\n",
      "50.00162554049221\n",
      "50.00487662147665\n",
      "50.00812770246107\n",
      "Yay 16000\n",
      "Yay 17000\n",
      "Yay 18000\n",
      "Yay 19000\n",
      "Yay 20000\n",
      "Yay 21000\n",
      "70.0022757566891\n",
      "70.00552683767353\n",
      "70.00877791865796\n",
      "Yay 22000\n",
      "Yay 23000\n",
      "Yay 24000\n",
      "Yay 25000\n",
      "Yay 26000\n",
      "Yay 27000\n",
      "90.00292597288599\n",
      "90.0061770538704\n",
      "90.00942813485484\n",
      "Yay 28000\n",
      "Yay 29000\n",
      "Yay 30000\n",
      "Done lemmatization\n",
      "Selection #3\n",
      "Training logistic regression\n",
      "Yay 0\n",
      "Yay 1000\n",
      "Yay 2000\n",
      "Yay 3000\n",
      "10.000325108098442\n",
      "10.003576189082871\n",
      "10.006827270067298\n",
      "Yay 4000\n",
      "Yay 5000\n",
      "Yay 6000\n",
      "Yay 7000\n",
      "Yay 8000\n",
      "Yay 9000\n",
      "30.000975324295325\n",
      "30.004226405279756\n",
      "30.007477486264182\n",
      "Yay 10000\n",
      "Yay 11000\n",
      "Yay 12000\n",
      "Yay 13000\n",
      "Yay 14000\n",
      "Yay 15000\n",
      "50.00162554049221\n",
      "50.00487662147665\n",
      "50.00812770246107\n",
      "Yay 16000\n",
      "Yay 17000\n",
      "Yay 18000\n",
      "Yay 19000\n",
      "Yay 20000\n",
      "Yay 21000\n",
      "70.0022757566891\n",
      "70.00552683767353\n",
      "70.00877791865796\n",
      "Yay 22000\n",
      "Yay 23000\n",
      "Yay 24000\n",
      "Yay 25000\n",
      "Yay 26000\n",
      "Yay 27000\n",
      "90.00292597288599\n",
      "90.0061770538704\n",
      "90.00942813485484\n",
      "Yay 28000\n",
      "Yay 29000\n",
      "Yay 30000\n",
      "Done lemmatization\n",
      "Selection #4\n",
      "Training logistic regression\n",
      "Yay 0\n",
      "Yay 1000\n",
      "Yay 2000\n",
      "Yay 3000\n",
      "10.000325108098442\n",
      "10.003576189082871\n",
      "10.006827270067298\n",
      "Yay 4000\n",
      "Yay 5000\n",
      "Yay 6000\n",
      "Yay 7000\n",
      "Yay 8000\n",
      "Yay 9000\n",
      "30.000975324295325\n",
      "30.004226405279756\n",
      "30.007477486264182\n",
      "Yay 10000\n",
      "Yay 11000\n",
      "Yay 12000\n",
      "Yay 13000\n",
      "Yay 14000\n",
      "Yay 15000\n",
      "50.00162554049221\n",
      "50.00487662147665\n",
      "50.00812770246107\n",
      "Yay 16000\n",
      "Yay 17000\n",
      "Yay 18000\n",
      "Yay 19000\n",
      "Yay 20000\n",
      "Yay 21000\n",
      "70.0022757566891\n",
      "70.00552683767353\n",
      "70.00877791865796\n",
      "Yay 22000\n",
      "Yay 23000\n",
      "Yay 24000\n",
      "Yay 25000\n",
      "Yay 26000\n",
      "Yay 27000\n",
      "90.00292597288599\n",
      "90.0061770538704\n",
      "90.00942813485484\n",
      "Yay 28000\n",
      "Yay 29000\n",
      "Yay 30000\n",
      "Done lemmatization\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopword\n",
    "\n",
    "\n",
    "def check_percentage(i, n):\n",
    "    r = i / n * 100\n",
    "    if r > 10 and  r < 10.01:\n",
    "        print(r)\n",
    "    elif r > 30 and  r < 30.01:\n",
    "        print(r)\n",
    "    elif r > 50 and  r < 50.01:\n",
    "        print(r)\n",
    "    elif r > 70 and  r < 70.01:\n",
    "        print(r)\n",
    "    elif r > 90 and  r < 90.01:\n",
    "        print(r)\n",
    "    \n",
    "stopwords_ru = [w for w in stopwords.words(\"russian\")]\n",
    "\n",
    "lin_reg_clf = Pipeline([('vect', CountVectorizer(stop_words = stopwords_ru)), \n",
    "             ('tfidf', TfidfTransformer()),\n",
    "             ('clf', LogisticRegression())\n",
    "               ])\n",
    "                \n",
    "lin_reg_models = []\n",
    "\n",
    "for i in range(selections_num - 1):\n",
    "    print(\"Selection #\" + str(i))\n",
    "    start = i * selection_size\n",
    "    end = start + selection_size\n",
    "    print(\"Training logistic regression\")\n",
    "    curr_texts = texts[start:end]\n",
    "    normalized_texts = []\n",
    "    \n",
    "    \n",
    "    n = len(curr_texts)\n",
    "    for i, text in enumerate(curr_texts):\n",
    "        check_percentage(i, n)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [morph.parse(t.lower())[0].normal_form for t in tokens if t.isalnum()]\n",
    "        normalized_texts.append(' '.join(tokens))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Yay \" + str(i))\n",
    "    print(\"Done lemmatization\")\n",
    "    \n",
    "    \n",
    "    lin_reg_models.append(lin_reg_clf.fit(normalized_texts, text_tags[start:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517067620286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "start = (selections_num - 1) * selection_size\n",
    "test_selection_cnt = corpus_size - (selections_num - 1) * selection_size\n",
    "test_tags = text_tags[start:]\n",
    "\n",
    "'''\n",
    "X_test_counts = count_vect.fit(texts[start:])\n",
    "X_test_tfidf = tfidf_transformer.fit(X_test_counts)\n",
    "'''\n",
    "\n",
    "lin_reg_predictions = [model.predict(texts[start:]) for model in lin_reg_models]\n",
    "prediction = []\n",
    "#multinom_predictions = [model.predict(X_test_tfidf) for model in multinom_models\n",
    "for i in range(len(lin_reg_predictions[0])):\n",
    "    predictions = [p[i] for p in lin_reg_predictions]\n",
    "    prediction.append(max(set(predictions), key=predictions.count))\n",
    "\n",
    "\n",
    "prediction = np.array(prediction)\n",
    "log_reg_accuracy = accuracy_score(test_tags, prediction)\n",
    "print(log_reg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('D:\\Max\\docs_cluster\\lenta\\lenta_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Деловой климат</td>\n",
       "      <td>Заместитель председателя правительства Аркадий...</td>\n",
       "      <td>Правительство прокомментировало идею о запрете...</td>\n",
       "      <td>Бизнес</td>\n",
       "      <td>https://lenta.ru/news/2017/04/01/24hours/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>События</td>\n",
       "      <td>Монреальская конвенция об унификации правил во...</td>\n",
       "      <td>Эксперт отвел год на окончательную ратификацию...</td>\n",
       "      <td>Путешествия</td>\n",
       "      <td>https://lenta.ru/news/2017/03/31/motrealwork/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Общество</td>\n",
       "      <td>Сотни байкеров в столице Аргентины Буэнос-Айре...</td>\n",
       "      <td>Аргентинские байкеры устроили акцию протеста п...</td>\n",
       "      <td>Мир</td>\n",
       "      <td>https://lenta.ru/news/2017/03/30/bikers/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tags                                               text  \\\n",
       "0  Деловой климат  Заместитель председателя правительства Аркадий...   \n",
       "1         События  Монреальская конвенция об унификации правил во...   \n",
       "2        Общество  Сотни байкеров в столице Аргентины Буэнос-Айре...   \n",
       "\n",
       "                                               title        topic  \\\n",
       "0  Правительство прокомментировало идею о запрете...       Бизнес   \n",
       "1  Эксперт отвел год на окончательную ратификацию...  Путешествия   \n",
       "2  Аргентинские байкеры устроили акцию протеста п...          Мир   \n",
       "\n",
       "                                             url  \n",
       "0      https://lenta.ru/news/2017/04/01/24hours/  \n",
       "1  https://lenta.ru/news/2017/03/31/motrealwork/  \n",
       "2       https://lenta.ru/news/2017/03/30/bikers/  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['tags', 'text', 'title', 'topic', 'url']\n",
    "tags = [' спецпроект',\n",
    " 'Coцсети',\n",
    " 'Авто',\n",
    " 'Автобизнес',\n",
    " 'Аналитика рынка',\n",
    " 'Архитектура',\n",
    " 'Банки',\n",
    " 'Баскетбол',\n",
    " 'Безопасность ',\n",
    " 'Белоруссия',\n",
    " 'Бокс и ММА',\n",
    " 'Вещи',\n",
    " 'Вирусные ролики',\n",
    " 'Вкусы',\n",
    " 'Внешний вид',\n",
    " 'Вооружение',\n",
    " 'Вооруженные силы',\n",
    " 'Все',\n",
    " 'Выборы',\n",
    " 'Гаджеты',\n",
    " 'Госрегулирование',\n",
    " 'Госэкономика',\n",
    " 'Движение',\n",
    " 'Деловой климат',\n",
    " 'Деньги',\n",
    " 'Достижения',\n",
    " 'Еда',\n",
    " 'Звери',\n",
    " 'Зимние виды',\n",
    " 'Игры',\n",
    " 'Инновации',\n",
    " 'Инструменты',\n",
    " 'Интернет',\n",
    " 'Искусство',\n",
    " 'История',\n",
    " 'Кавказ',\n",
    " 'Казахстан',\n",
    " 'Катастрофы',\n",
    " 'Кино',\n",
    " 'Книги',\n",
    " 'Компании',\n",
    " 'Конфликты',\n",
    " 'Космос',\n",
    " 'Криминал',\n",
    " 'Крым',\n",
    " 'Культпросвет',\n",
    " 'Легпром',\n",
    " 'Летние виды',\n",
    " 'Люди',\n",
    " 'Мемы',\n",
    " 'Мир',\n",
    " 'Мировой бизнес',\n",
    " 'Мировой опыт',\n",
    " 'Мнения',\n",
    " 'Молдавия',\n",
    " 'Москва',\n",
    " 'Музыка',\n",
    " 'Наследие',\n",
    " 'Наука',\n",
    " 'Общество',\n",
    " 'Оружие',\n",
    " 'Первая мировая',\n",
    " 'Политика',\n",
    " 'Полиция и спецслужбы',\n",
    " 'Пресса',\n",
    " 'Преступность',\n",
    " 'Прибалтика',\n",
    " 'Производители',\n",
    " 'Происшествия',\n",
    " 'Регионы',\n",
    " 'Реклама',\n",
    " 'Ресурсы',\n",
    " 'Россия',\n",
    " 'Рынки',\n",
    " 'Следствие и суд',\n",
    " 'События',\n",
    " 'Средняя Азия',\n",
    " 'Стиль',\n",
    " 'Страноведение',\n",
    " 'ТВ и радио',\n",
    " 'Театр',\n",
    " 'Теннис',\n",
    " 'Техника',\n",
    " 'Технологии',\n",
    " 'Туризм',\n",
    " 'Украина',\n",
    " 'Финансы компаний',\n",
    " 'Футбол',\n",
    " 'Хоккей',\n",
    " 'Часы',\n",
    " 'Экология',\n",
    " 'Явления']\n",
    "\n",
    "num_tags = [{'tag': tag, 'id': i} for i, tag in enumerate(tags)]\n",
    "\n",
    "texts = []\n",
    "text_tags = []\n",
    "\n",
    "with codecs.open('D:\\Max\\docs_cluster\\lenta\\lenta_data.csv', 'r', 'utf_8_sig') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        texts.append(row[1])\n",
    "        text_tags.append(next((tag['id'] for tag in num_tags if tag['tag'] == row[0]), None))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
